---
title: 'AE 20: Multiple regression I'
author: "Dav King"
date: "3/26/2022"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r, warning = FALSE, message  = FALSE}
library(tidyverse)
library(tidymodels)
library(scatterplot3d)
```

# Reminder

- Lab Due on Friday at 11:59 PM.

- Discussion of Peer Review.

# Learning goals

- Gain proficiency with multiple linear regression
- Review what a dummy variable is

To begin, we'll work with a dataset with information on the price of sports cars new set of pokemon data.

```{r load-data, message = F}
sports_car_prices <- read_csv("sportscars.csv")
pokemon <- read_csv("pokemon150.csv")
```

## The linear model with one predictor

- Previously, we were interested in the $$\beta_0$$ (population parameter for the intercept)
and the $$\beta_1$$ (population parameter for the slope) in the following model:

$$ \hat{y} = \beta_0 + \beta_1~x + \epsilon $$

- Unfortunately, we can't get these values

- So we use sample statistics to estimate them:

$$ \hat{y} = b_0 + b_1~x $$

## The linear model with multiple predictors

$$ \hat{y} = \beta_0 + \beta_1~x_1 + \beta_2~x_2 + \cdots + \beta_k~x_k +\epsilon $$
- Sample model that we use to estimate the population model:
  
$$ \hat{y} = b_0 + b_1~x_1 + b_2~x_2 + \cdots + b_k~x_k $$

## An example

The file `sportscars.csv` contains prices for Porsche and Jaguar cars for sale
on cars.com.

`car`: car make (Jaguar or Porsche)

`price`: price in USD

`age`: age of the car in years

`mileage`: previous miles driven

## The linear model with a single predictor

```{r pricesmodel}
prices_model <- linear_reg() %>%
  set_engine("lm") %>%
  fit(price ~ age, data = sports_car_prices)
tidy(prices_model)
```

But is the age the only variable that predicts price?

## The linear model with multiple predictors

$$ \hat{y} = \beta_0 + \beta_1~x_1 + \beta_2~x_2 + \cdots + \beta_k~x_k +\epsilon $$

- Sample model that we use to estimate the population model:
  
$$ \hat{y} = b_0 + b_1~x_1 + b_2~x_2 + \cdots + b_k~x_k $$

Let's add a variable. 

## Multiple Regression 

```{r maineffects}
m_main <- linear_reg() %>%
  set_engine("lm") %>%
  fit(price ~ age + car, data = sports_car_prices)
m_main %>%
  tidy() %>%
  select(term, estimate)
```

Linear model:

$$ \widehat{price} = 44310 - 2487~age + 21648~carPorsche $$


- Plug in 0 for `carPorsche` to get the linear model for Jaguars.

- Plug in 1 for `carPorsche` to get the linear model for Porsches.

- Jaguar: 

$$ \widehat{price} = 44310 - 2487~age + 21648* 0 \\
=44310-2487*age$$

- Porsche: 
$$ \widehat{price} = 44310 - 2487~age + 21648* 1 \\
=65958 - 2487~age$$

- Rate of change in price as the age of the car increases does not depend on make of car (same slopes)
- Porsches are consistently more expensive than Jaguars (different intercepts)

## Main effects, numerical and categorical predictors

```{r maineffects2}
m_main %>%
  tidy() %>%
  select(term, estimate)
```

```{r maincoefs}
m_main_coefs <- m_main %>%
  tidy() %>%
  select(term, estimate)
m_main_coefs
```

- **All else held constant**, for each additional year of a car's age, the price
of the car is predicted to decrease, on average, by $2,487.

- **All else held constant**, Porsches are predicted, on average, to have a 
price that is $21,648 greater than Jaguars.

- Jaguars that have an age of 0 are predicted, on average, to have a price of $44,310.

# Adjusted R-Squared

- The strength of the fit of a linear model is commonly evaluated using $R^2$.

- It tells us what percentage of the variability in the response variable is explained by the model. The remainder of the variability is unexplained.

## Please recall:

- We can write explained variation using the following ratio of sums of squares:

$$ R^2 =  1 - \left( \frac{ SS\_{Error} }{ SS\_{Total} } \right) $$

where $SS_{Error}$ is the sum of squared residuals and $SS_{Total}$ is the total
variance in the response variable.

## Adjusted $R^2$

$$ R^2\_{adj} = 1 - \left( \frac{ SS\_{Error} }{ SS\_{Total} } \times \frac{n - 1}{n - k - 1} \right), $$

where $n$ is the number of observations and $k$ is the number of predictors in 
the model.

- Adjusted $R^2$ doesn't increase if the new variable does not provide any new 
information or is completely unrelated and can even decrease.

- This makes adjusted $R^2$ a preferable metric for model selection in multiple
regression models.

Let's find the $R^2$ and adjusted $R^2$ for the `m_main` model we built.

```{r adj-r2}
glance(m_main)$r.squared
glance(m_main)$adj.r.squared
```

# Exercises

Now, let's do some exercises with the Pokemon data.

## Exercise 1)

Are height and weight correlated with a Pokemon's hit points? Run a bivariate linear regression model for each of these. Do you find statistically significant results?

```{r bivariate}
hp_height <- linear_reg() %>% 
  set_engine("lm") %>% 
  fit(hp ~ height_m, data = pokemon)
hp_weight <- linear_reg() %>% 
  set_engine("lm") %>% 
  fit(hp ~ weight_kg, data = pokemon)
tidy(hp_height)
tidy(hp_weight)
```

Yes, for each of these P is (much) less than 0.05; thus, we can reject $H_0$.

## Exercise 2) 

Other variables may be correlated with `hp`, e.g. a pokemon's legendary status. 

Do legendary pokemon have higher `hp` than non-legendary pokemon? Compare mean `hp` between groups to support your answer.

```{r hp-vs-legendary}
x <- pokemon %>% 
  filter(is_legendary == 0) %>% 
  pull(hp)
y <- pokemon %>% 
  filter(is_legendary == 1) %>% 
  pull(hp)
t.test(x, y, var.equal = T, alternative = "less")
#Even if you wanted me to be boring and just compare the means of the two 
#groups without a statistical test, this model still puts them out
```

Yes, legendary pokemon have a higher `hp`. Their mean `hp` is 108.429, while 
the mean `hp` of non-legendary pokemon is 66.420 - a difference with a p-value
of essentially zero.

Write down a model to predict a pokemon's hitpoints based on their height, weight, legendary status (use $x$, $y$, $\beta$ notation). Define each variable.

$$\hat{y} = b_0 + b_1x_1 + b_2x_2 +b_3x_3$$
Where $\hat{y}$ is our predicted hp, $b_0$ is our predicted hp for a 
non-legendary pokemon that is 0 meters tall and weighs 0 kilograms, $b_1$ is 
the amount that each unit increase in height $x_1$ increases the pokemon's 
predicted hp, $b_2$ is the amount that each unit increase in weight $x_2$ 
increases the pokemon's predicted hp, and $b_3$ is the amount that a pokemon 
being legendary ($x_3$) increases the pokemon's predicted hp.

## Exercise 3) 

Use `tidymodel` syntax to build a linear model with *all three variables* and estimate each $\beta$. Then, find and interpret the adjusted $R^2$.

```{r hp-lm}
hp_model <- linear_reg() %>% 
  set_engine("lm") %>% 
  fit(hp ~ height_m + weight_kg + is_legendary, data = pokemon)
tidy(hp_model)
glance(hp_model)$adj.r.squared
```

Interpret the meaning of your estimates and write a brief description below.

Intercept: 55.9: we expect a non-legendary pokemon with height and weight 0 
to have 55.9 hp.

Slopes: 

  - 5.10: for each additional meter of height, we predict a corresponding 
    increase of 5.10 hp for the pokemon.
    
  - 0.079: for each additional kilogram of weight, we predict a corresponding
    increase of 0.079 hp for the pokemon.
    
  = 5.31: we predict that being legendary will increase a pokemon's hp by 5.31.

Adjusted $R^2$: 0.33: 33% of the variance in Pokemon hp can be explained by its
height, weight, and legendary status.

## Exercise 4)

Some think that certain pokemon types have higher `hp` than others.

First, explore where there is a different `mean_hp` by `type_1`.

```{r hp-vs-type}
pokemon %>% 
  group_by(type_1) %>% 
  summarize(mean_hp = mean(hp))

hp_means <- aov(hp ~ type_1, data = pokemon)
tidy(hp_means)
```

While it is clear from just the means themselves that there is some nominal 
difference in means by the type of pokemon, it is not statistically significant 
according to this one-way ANOVA test.

Then, please construct a linear model in `R` to determine the effect of pokemon type on `hp`. 

```{r hp-type-lm}
type_model <- linear_reg() %>% 
  set_engine("lm") %>% 
  fit(hp ~ type_1, data = pokemon)
tidy(type_model)
```


Interpret the meaning of your estimates and write a brief description below. Are any types missing?

One type is missing - bug, the first alphabetically and lowest in terms of hp,
which reflects our intercept. We expect a bug type pokemon to have an hp of 
58.5, and we expect the hp of each other type of pokemon to increase above that 
by its `estimate` given in the table.